{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# AI Security Agent - Advanced 6-Stage Distillation Pipeline\n",
                "# Unsloth + Llama-3 8B for Specialized Security SLMs\n",
                "\n",
                "This notebook implements the full distillation sequence:\n",
                "1. **Teacher Reasoning**: CoT Data Generation\n",
                "2. **Reasoning Distillation**: Training the student SLM\n",
                "3. **Safety Alignment**: DPO (Direct Preference Optimization)\n",
                "4. **Multi-Stage Quantization**: INT4/GGUF/Merged Export\n",
                "5. **Hugging Face Integration**: Saving and Loading from Hub"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### CELL 1: Installation\n",
                "âš ï¸ IMPORTANT: Use Colab with GPU (T4, A100, or V100)\n",
                "Runtime â†’ Change runtime type â†’ GPU"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -q -U \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
                "!pip install -q -U torch transformers datasets trl peft accelerate bitsandbytes huggingface_hub wandb\n",
                "\n",
                "print(\"âœ… Installation complete!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### CELL 2: Imports & Login"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import json\n",
                "import os\n",
                "from datasets import Dataset, load_dataset\n",
                "from transformers import TrainingArguments, TextStreamer\n",
                "from trl import SFTTrainer\n",
                "from unsloth import FastLanguageModel\n",
                "from huggingface_hub import notebook_login\n",
                "import pandas as pd\n",
                "from typing import Dict, List\n",
                "import random\n",
                "\n",
                "# Login to Hugging Face\n",
                "# notebook_login()\n",
                "\n",
                "# Check GPU\n",
                "print(f\"GPU Available: {torch.cuda.is_available()}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
                "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### CELL 3: Stage 2 - Advanced CoT Data Generation\n",
                "Simulating Teacher reasoning to teach the Student SLM *how* to think about security threats."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def generate_cot_security_data(role='red', num_samples=100):\n",
                "    \"\"\"Generates high-quality CoT data for either Red or Blue team.\"\"\"\n",
                "    red_categories = [\n",
                "        {\n",
                "            \"category\": \"indirect_prompt_injection\",\n",
                "            \"scenario\": \"Attacker hides malicious instructions in a document retrieved by RAG.\",\n",
                "            \"steps\": [\n",
                "                \"Detect RAG dependency.\",\n",
                "                \"Craft payload in reference text.\",\n",
                "                \"Semantically align payload with search query.\",\n",
                "                \"Execute hidden command after retrieval.\"\n",
                "            ]\n",
                "        }\n",
                "    ]\n",
                "    \n",
                "    blue_categories = [\n",
                "        {\n",
                "            \"category\": \"rag_integrity_check\",\n",
                "            \"scenario\": \"Implementing verification layer for RAG results.\",\n",
                "            \"steps\": [\n",
                "                \"Intercept retrieval stream.\",\n",
                "                \"Apply command-detection filter.\",\n",
                "                \"Sanitize instructions.\",\n",
                "                \"Enforce passive information processing.\"\n",
                "            ]\n",
                "        }\n",
                "    ]\n",
                "\n",
                "    selected = red_categories if role == 'red' else blue_categories\n",
                "    data = []\n",
                "    for _ in range(num_samples):\n",
                "        cat = random.choice(selected)\n",
                "        reasoning = \"\\n\".join([f\"{i+1}. {s}\" for i, s in enumerate(cat['steps'])])\n",
                "        data.append({\n",
                "            \"instruction\": f\"You are a {'Red' if role=='red' else 'Blue'} Team Expert. Analyze the scenario.\",\n",
                "            \"input\": f\"Scenario: {cat['scenario']}\",\n",
                "            \"output\": json.dumps({\"thought_process\": reasoning, \"action\": \"Analyze and Propose\"}, indent=2)\n",
                "        })\n",
                "    return data\n",
                "\n",
                "sample_data = generate_cot_security_data('red', 50) + generate_cot_security_data('blue', 50)\n",
                "print(f\"âœ… Generated {len(sample_data)} CoT training samples\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### CELL 4: Format Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def format_alpaca_prompt(sample: Dict) -> str:\n",
                "    return f\"\"\"### Instruction:\\n{sample['instruction']}\\n\\n### Input:\\n{sample['input']}\\n\\n### Expert Reasoning:\\n{sample['output']}\"\"\"\n",
                "\n",
                "formatted_samples = [{\"text\": format_alpaca_prompt(s)} for s in sample_data]\n",
                "dataset = Dataset.from_list(formatted_samples).train_test_split(test_size=0.1, seed=42)\n",
                "print(f\"âœ… Training samples (Stage 3): {len(dataset['train'])}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### CELL 5: Load Model with Unsloth"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "max_seq_length = 2048\n",
                "model, tokenizer = FastLanguageModel.from_pretrained(\n",
                "    model_name = \"unsloth/Meta-Llama-3.1-8B\",\n",
                "    max_seq_length = max_seq_length,\n",
                "    dtype = None,\n",
                "    load_in_4bit = True,\n",
                ")\n",
                "print(\"âœ… Base model loaded!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### CELL 6: Apply LoRA Adapters for Distillation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model = FastLanguageModel.get_peft_model(\n",
                "    model,\n",
                "    r = 128, # Higher rank for capture logic\n",
                "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
                "    lora_alpha = 32,\n",
                "    lora_dropout = 0,\n",
                "    bias = \"none\",\n",
                "    use_gradient_checkpointing = \"unsloth\",\n",
                "    random_state = 42,\n",
                ")\n",
                "print(\"âœ… LoRA adapters applied for Distillation!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### CELL 7: Configure Training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "trainer = SFTTrainer(\n",
                "    model = model,\n",
                "    tokenizer = tokenizer,\n",
                "    train_dataset = dataset[\"train\"],\n",
                "    eval_dataset = dataset[\"test\"],\n",
                "    dataset_text_field = \"text\",\n",
                "    max_seq_length = max_seq_length,\n",
                "    args = TrainingArguments(\n",
                "        per_device_train_batch_size = 2,\n",
                "        gradient_accumulation_steps = 4,\n",
                "        warmup_steps = 5,\n",
                "        max_steps = 60,\n",
                "        learning_rate = 2e-4,\n",
                "        fp16 = not torch.cuda.is_bf16_supported(),\n",
                "        bf16 = torch.cuda.is_bf16_supported(),\n",
                "        logging_steps = 1,\n",
                "        optim = \"adamw_8bit\",\n",
                "        weight_decay = 0.01,\n",
                "        lr_scheduler_type = \"linear\",\n",
                "        seed = 42,\n",
                "        output_dir = \"outputs\",\n",
                "    ),\n",
                ")\n",
                "print(\"âœ… Training configured!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### CELL 8: Train"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "trainer.train()\n",
                "print(\"âœ… Training complete!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### CELL 9: Save & Quantize"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model.save_pretrained(\"slm-security-distilled\")\n",
                "tokenizer.save_pretrained(\"slm-security-distilled\")\n",
                "\n",
                "# --- STAGE 6: Quantization Export ---\n",
                "# model.save_pretrained_gguf(\"model_q4_k_m.gguf\", tokenizer, quantization_method = \"q4_k_m\")\n",
                "# model.save_pretrained_merged(\"model_merged_4bit\", tokenizer, save_method = \"merged_4bit\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### CELL 10: Hugging Face Integration - Push to Hub\n",
                "Push your distilled model to the Hugging Face Hub for sharing and easy deployment."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# REPLACE with your Hub username and model name\n",
                "HUB_MODEL_ID = \"your-username/slm-security-distilled\"\n",
                "\n",
                "print(f\"ðŸš€ Pushing model to Hugging Face Hub: {HUB_MODEL_ID}\")\n",
                "# model.push_to_hub(HUB_MODEL_ID, use_temp_dir=False)\n",
                "# tokenizer.push_to_hub(HUB_MODEL_ID, use_temp_dir=False)\n",
                "print(\"âœ… Push complete (Uncomment cells to execute)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### CELL 11: Inference Test & Load from Hub"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "FastLanguageModel.for_inference(model)\n",
                "inputs = tokenizer(\n",
                "[ \n",
                "    format_alpaca_prompt({\n",
                "        \"instruction\": \"You are a Red Team Expert. Analyze the scenario.\",\n",
                "        \"input\": \"Scenario: Attacker hides malicious instructions in a document retrieved by RAG.\",\n",
                "        \"output\": \"\"\n",
                "    })\n",
                "], return_tensors = \"pt\").to(\"cuda\")\n",
                "\n",
                "outputs = model.generate(**inputs, max_new_tokens = 128)\n",
                "print(tokenizer.batch_decode(outputs)[0])\n",
                "\n",
                "print(\"\\n--- To load from Hub later ---\")\n",
                "print(f\"# model, tokenizer = FastLanguageModel.from_pretrained(model_name = '{HUB_MODEL_ID}', load_in_4bit = True)\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}