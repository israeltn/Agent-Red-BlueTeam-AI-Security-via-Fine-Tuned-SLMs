{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Security Agent - Advanced 6-Stage Distillation Pipeline\n",
    "# Unsloth + Llama-3 8B for Specialized Security SLMs\n",
    "\n",
    "This notebook implements the full distillation sequence:\n",
    "1. **Teacher Reasoning**: CoT Data Generation\n",
    "2. **Reasoning Distillation**: Training the student SLM\n",
    "3. **Safety Alignment**: DPO (Direct Preference Optimization)\n",
    "4. **Multi-Stage Quantization**: INT4/GGUF/Merged Export\n",
    "5. **Hugging Face Integration**: Saving and Loading from Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CELL 1: Installation\n",
    "\u26a0\ufe0f IMPORTANT: Use Colab with GPU (T4, A100, or V100)\n",
    "Runtime \u2192 Change runtime type \u2192 GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install -q -U torch transformers datasets trl peft accelerate bitsandbytes huggingface_hub wandb\n",
    "\n",
    "print(\"\u2705 Installation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CELL 2: Imports & Login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import os\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import TrainingArguments, TextStreamer\n",
    "from trl import SFTTrainer\n",
    "from unsloth import FastLanguageModel\n",
    "from huggingface_hub import notebook_login\n",
    "import pandas as pd\n",
    "from typing import Dict, List\n",
    "import random\n",
    "\n",
    "# Login to Hugging Face\n",
    "# notebook_login()\n",
    "\n",
    "# Check GPU\n",
    "print(f\"GPU Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CELL 3: Load Final Datasets\n",
    "Upload `blue_team_data.json` and `red_team_data.json` to the Colab environment (click the folder icon on the left -> Upload)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from datasets import Dataset\n",
    "\n",
    "def load_local_dataset(file_path):\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"\u26a0\ufe0f Warning: {file_path} not found. Please upload it.\")\n",
    "        return []\n",
    "    with open(file_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Load both datasets\n",
    "blue_data = load_local_dataset('blue_team_data.json')\n",
    "red_data = load_local_dataset('red_team_data.json')\n",
    "\n",
    "combined_data = blue_data + red_data\n",
    "print(f\"\u2705 Loaded {len(blue_data)} Blue Team samples\")\n",
    "print(f\"\u2705 Loaded {len(red_data)} Red Team samples\")\n",
    "print(f\"\u2705 Total samples: {len(combined_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CELL 4: Format Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_alpaca_prompt(sample: Dict) -> str:\n",
    "    return f\"\"\"### Instruction:\\n{sample['instruction']}\\n\\n### Input:\\n{sample['input']}\\n\\n### Response:\\n{sample['output']}\"\"\"\n",
    "\n",
    "formatted_samples = [{\"text\": format_alpaca_prompt(s)} for s in combined_data]\n",
    "dataset = Dataset.from_list(formatted_samples).train_test_split(test_size=0.1, seed=42)\n",
    "print(f\"\u2705 Training samples: {len(dataset['train'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CELL 5: Load Model with Unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 2048\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Meta-Llama-3.1-8B\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = None,\n",
    "    load_in_4bit = True,\n",
    ")\n",
    "print(\"\u2705 Base model loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CELL 6: Apply LoRA Adapters for Distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 128, # Higher rank for capture logic\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha = 32,\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    random_state = 42,\n",
    ")\n",
    "print(\"\u2705 LoRA adapters applied for Distillation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CELL 7: Configure Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset[\"train\"],\n",
    "    eval_dataset = dataset[\"test\"],\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        max_steps = 60,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not torch.cuda.is_bf16_supported(),\n",
    "        bf16 = torch.cuda.is_bf16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 42,\n",
    "        output_dir = \"outputs\",\n",
    "    ),\n",
    ")\n",
    "print(\"\u2705 Training configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CELL 8: Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "print(\"\u2705 Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CELL 9: Save & Quantize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"slm-security-distilled\")\n",
    "tokenizer.save_pretrained(\"slm-security-distilled\")\n",
    "\n",
    "# --- STAGE 6: Quantization Export ---\n",
    "# model.save_pretrained_gguf(\"model_q4_k_m.gguf\", tokenizer, quantization_method = \"q4_k_m\")\n",
    "# model.save_pretrained_merged(\"model_merged_4bit\", tokenizer, save_method = \"merged_4bit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CELL 10: Hugging Face Integration - Push to Hub\n",
    "Push your distilled model to the Hugging Face Hub for sharing and easy deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "import os\n",
    "\n",
    "# REPLACE with your Hub username and model name\n",
    "HUB_MODEL_ID = \"your-username/ai-security-slm-llama3\"\n",
    "\n",
    "print(f\"\ud83d\ude80 Saving model locally...\")\n",
    "model.save_pretrained(\"ai-security-slm-local\")\n",
    "tokenizer.save_pretrained(\"ai-security-slm-local\")\n",
    "\n",
    "print(f\"\ud83d\ude80 Pushing model to Hugging Face Hub: {HUB_MODEL_ID}\")\n",
    "# Use Colab Secrets for your token or notebook_login()\n",
    "try:\n",
    "    hf_token = userdata.get('HF_TOKEN')\n",
    "except:\n",
    "    hf_token = None\n",
    "\n",
    "model.push_to_hub(HUB_MODEL_ID, token=hf_token)\n",
    "tokenizer.push_to_hub(HUB_MODEL_ID, token=hf_token)\n",
    "print(\"\u2705 Save and Push complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CELL 11: Inference Test & Load from Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model)\n",
    "inputs = tokenizer(\n",
    "[ \n",
    "    format_alpaca_prompt({\n",
    "        \"instruction\": \"You are a Red Team Expert. Analyze the scenario.\",\n",
    "        \"input\": \"Scenario: Attacker hides malicious instructions in a document retrieved by RAG.\",\n",
    "        \"output\": \"\"\n",
    "    })\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens = 128)\n",
    "print(tokenizer.batch_decode(outputs)[0])\n",
    "\n",
    "print(\"\\n--- To load from Hub later ---\")\n",
    "print(f\"# model, tokenizer = FastLanguageModel.from_pretrained(model_name = '{HUB_MODEL_ID}', load_in_4bit = True)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}